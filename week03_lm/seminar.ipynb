{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "k1gpzj4guo8e1riwj3om1k"
   },
   "source": [
    "### N-gram language models or how to write scientific papers (4 pts)\n",
    "\n",
    "We shall train our language model on a corpora of [ArXiv](http://arxiv.org/) articles and see if we can generate a new one!\n",
    "\n",
    "![img](https://media.npr.org/assets/img/2013/12/10/istock-18586699-monkey-computer_brick-16e5064d3378a14e0e4c2da08857efe03c04695e-s800-c85.jpg)\n",
    "\n",
    "_data by neelshah18 from [here](https://www.kaggle.com/neelshah18/arxivdataset/)_\n",
    "\n",
    "_Disclaimer: this has nothing to do with actual science. But it's fun, so who cares?!_"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "u8jdaiy68oib3jvr4k01",
    "ExecuteTime": {
     "end_time": "2024-12-29T09:34:22.261124Z",
     "start_time": "2024-12-29T09:34:21.398627Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "0c76vnyl3zui9yhtkodgrlf",
    "ExecuteTime": {
     "end_time": "2024-12-29T09:34:23.707394Z",
     "start_time": "2024-12-29T09:34:23.196842Z"
    }
   },
   "source": [
    "# Alternative manual download link: https://yadi.sk/d/_nGyU2IajjR9-w\n",
    "# !wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
    "# !tar -xvzf arxivData.json.tar.gz\n",
    "data = pd.read_json(\"./arxivData.json\")\n",
    "data.sample(n=5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  author  day            id  \\\n",
       "24549  [{'name': 'Lee-Kang Liu'}, {'name': 'Stanley H...   14   1407.3840v4   \n",
       "40117                  [{'name': 'Christian S. Perone'}]   11  1711.04069v1   \n",
       "26189  [{'name': 'Toni Heidenreich'}, {'name': 'Micha...    7  1609.01932v1   \n",
       "19805  [{'name': 'Zhengzhu Feng'}, {'name': 'Shlomo Z...   11   1207.4116v1   \n",
       "35186  [{'name': 'Joseph Y. Halpern'}, {'name': 'Yora...    2  cs/0006009v1   \n",
       "\n",
       "                                                    link  month  \\\n",
       "24549  [{'rel': 'alternate', 'href': 'http://arxiv.or...      7   \n",
       "40117  [{'rel': 'alternate', 'href': 'http://arxiv.or...     11   \n",
       "26189  [{'rel': 'alternate', 'href': 'http://arxiv.or...      9   \n",
       "19805  [{'rel': 'alternate', 'href': 'http://arxiv.or...      7   \n",
       "35186  [{'rel': 'alternate', 'href': 'http://arxiv.or...      6   \n",
       "\n",
       "                                                 summary  \\\n",
       "24549  The rapid development of 3D technology and com...   \n",
       "40117  In this work, we propose a straightforward met...   \n",
       "26189  Visual speech recognition aims to identify the...   \n",
       "19805  We present a major improvement to the incremen...   \n",
       "35186  Reasoning about knowledge seems to play a fund...   \n",
       "\n",
       "                                                     tag  \\\n",
       "24549  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
       "40117  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
       "26189  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
       "19805  [{'term': 'cs.AI', 'scheme': 'http://arxiv.org...   \n",
       "35186  [{'term': 'cs.DC', 'scheme': 'http://arxiv.org...   \n",
       "\n",
       "                                                   title  year  \n",
       "24549  Depth Reconstruction from Sparse Samples: Repr...  2014  \n",
       "40117  Towards ECDSA key derivation from deep embeddi...  2017  \n",
       "26189  A three-dimensional approach to Visual Speech ...  2016  \n",
       "19805        Region-Based Incremental Pruning for POMDPs  2012  \n",
       "35186  Knowledge and common knowledge in a distribute...  2000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>day</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>month</th>\n",
       "      <th>summary</th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24549</th>\n",
       "      <td>[{'name': 'Lee-Kang Liu'}, {'name': 'Stanley H...</td>\n",
       "      <td>14</td>\n",
       "      <td>1407.3840v4</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>7</td>\n",
       "      <td>The rapid development of 3D technology and com...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Depth Reconstruction from Sparse Samples: Repr...</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40117</th>\n",
       "      <td>[{'name': 'Christian S. Perone'}]</td>\n",
       "      <td>11</td>\n",
       "      <td>1711.04069v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>11</td>\n",
       "      <td>In this work, we propose a straightforward met...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Towards ECDSA key derivation from deep embeddi...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26189</th>\n",
       "      <td>[{'name': 'Toni Heidenreich'}, {'name': 'Micha...</td>\n",
       "      <td>7</td>\n",
       "      <td>1609.01932v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>9</td>\n",
       "      <td>Visual speech recognition aims to identify the...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>A three-dimensional approach to Visual Speech ...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19805</th>\n",
       "      <td>[{'name': 'Zhengzhu Feng'}, {'name': 'Shlomo Z...</td>\n",
       "      <td>11</td>\n",
       "      <td>1207.4116v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>7</td>\n",
       "      <td>We present a major improvement to the incremen...</td>\n",
       "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Region-Based Incremental Pruning for POMDPs</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35186</th>\n",
       "      <td>[{'name': 'Joseph Y. Halpern'}, {'name': 'Yora...</td>\n",
       "      <td>2</td>\n",
       "      <td>cs/0006009v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>6</td>\n",
       "      <td>Reasoning about knowledge seems to play a fund...</td>\n",
       "      <td>[{'term': 'cs.DC', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Knowledge and common knowledge in a distribute...</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "lbyqb5rx7j8jpo591r06ak",
    "ExecuteTime": {
     "end_time": "2024-12-29T09:34:25.549144Z",
     "start_time": "2024-12-29T09:34:25.191085Z"
    }
   },
   "source": [
    "# assemble lines: concatenate title and description\n",
    "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'].replace(\"\\n\", ' '), axis=1).tolist()\n",
    "\n",
    "sorted(lines, key=len)[:3]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
       " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
       " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7u97m5s8ekl5zd5a43a1yc"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "You know the dril. The data is messy. Go clean the data. Use WordPunctTokenizer or something.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "u8rvfk719iek97t3rarwr",
    "ExecuteTime": {
     "end_time": "2024-12-29T09:34:35.449441Z",
     "start_time": "2024-12-29T09:34:32.108620Z"
    }
   },
   "source": [
    "# Task: convert lines (in-place) into strings of space-separated tokens. Import & use WordPunctTokenizer\n",
    "from nltk import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "lines = [\" \".join(tokenizer.tokenize(line.lower())) for line in lines]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "w88nddpp2k8edoeyyyjh0l",
    "ExecuteTime": {
     "end_time": "2024-12-29T09:34:36.045392Z",
     "start_time": "2024-12-29T09:34:36.020956Z"
    }
   },
   "source": [
    "assert sorted(lines, key=len)[0] == \\\n",
    "    'differential contrastive divergence ; this paper has been retracted .'\n",
    "assert sorted(lines, key=len)[2] == \\\n",
    "    'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "qb6h3hxmr095egzv8rlzul"
   },
   "source": [
    "### N-Gram Language Model (1point)\n",
    "\n",
    "A language model is a probabilistic model that estimates text probability: the joint probability of all tokens $w_t$ in text $X$: $P(X) = P(w_1, \\dots, w_T)$.\n",
    "\n",
    "It can do so by following the chain rule:\n",
    "$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$ \n",
    "\n",
    "The problem with such approach is that the final term $P(w_T \\mid w_1, \\dots, w_{T-1})$ depends on $n-1$ previous words. This probability is impractical to estimate for long texts, e.g. $T = 1000$.\n",
    "\n",
    "One popular approximation is to assume that next word only depends on a finite amount of previous words:\n",
    "\n",
    "$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})$$\n",
    "\n",
    "Such model is called __n-gram language model__ where n is a parameter. For example, in 3-gram language model, each word only depends on 2 previous words. \n",
    "\n",
    "$$\n",
    "    P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n",
    "$$\n",
    "\n",
    "You can also sometimes see such approximation under the name of _n-th order markov assumption_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "u68wydbiioqlp5gl96mhd"
   },
   "source": [
    "The first stage to building such a model is counting all word occurences given N-1 previous words"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T09:35:22.601010Z",
     "start_time": "2024-12-29T09:35:22.592704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "og84gjipnumsakhiiu9ap",
    "ExecuteTime": {
     "end_time": "2024-12-29T12:03:15.097290Z",
     "start_time": "2024-12-29T12:03:15.092385Z"
    }
   },
   "source": [
    "# special tokens: \n",
    "# - `UNK` represents absent tokens, \n",
    "# - `EOS` is a special token after the end of sequence\n",
    "\n",
    "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
    "\n",
    "def count_ngrams(lines, n, tqdm_desc=\"Counting ngrams\"):\n",
    "    \"\"\"\n",
    "    Count how many times each word occured after (n - 1) previous words\n",
    "    :param lines: an iterable of strings with space-separated tokens\n",
    "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
    "\n",
    "    When building counts, please consider the following two edge cases:\n",
    "    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n",
    "      empty prefix: \"\" -> (UNK, UNK)\n",
    "      short prefix: \"the\" -> (UNK, the)\n",
    "      long prefix: \"the new approach\" -> (new, approach)\n",
    "    - you should add a special token, EOS, at the end of each sequence\n",
    "      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n",
    "      count the probability of this token just like all others.\n",
    "    \"\"\"\n",
    "    counts = defaultdict(Counter)\n",
    "    # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n",
    "    for line in tqdm(lines, desc=tqdm_desc):\n",
    "        queue = [UNK]*(n-1)\n",
    "        for word in (*line.split(), EOS):\n",
    "            counts[tuple(queue)][word] += 1\n",
    "            queue.append(word)\n",
    "            queue.pop(0)\n",
    "    \n",
    "    return counts\n"
   ],
   "outputs": [],
   "execution_count": 197
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "xyf2he6lak9mmqarl3nck",
    "ExecuteTime": {
     "end_time": "2024-12-29T09:35:46.864939Z",
     "start_time": "2024-12-29T09:35:46.841874Z"
    }
   },
   "source": [
    "# let's test it\n",
    "dummy_lines = sorted(lines, key=len)[:100]\n",
    "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
    "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
    "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
    "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
    "assert dummy_counts['p', '=']['np'] == 2\n",
    "assert dummy_counts['author', '.']['_EOS_'] == 1"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 19029.55it/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "4j620npeqvj0k8ak8xqx8xk"
   },
   "source": [
    "Once we can count N-grams, we can build a probabilistic language model.\n",
    "The simplest way to compute probabilities is in proporiton to counts:\n",
    "\n",
    "$$ P(w_t | prefix) = { Count(prefix, w_t) \\over \\sum_{\\hat w} Count(prefix, \\hat w) } $$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "c7cm76wmzlaa12bctznzei",
    "ExecuteTime": {
     "end_time": "2024-12-29T09:36:39.212031Z",
     "start_time": "2024-12-29T09:36:39.205352Z"
    }
   },
   "source": [
    "class NGramLanguageModel:    \n",
    "    def __init__(self, lines, n):\n",
    "        \"\"\" \n",
    "        Train a simple count-based language model: \n",
    "        compute probabilities P(w_t | prefix) given ngram counts\n",
    "        \n",
    "        :param n: computes probability of next token given (n - 1) previous words\n",
    "        :param lines: an iterable of strings with space-separated tokens\n",
    "        \"\"\"\n",
    "        assert n >= 1\n",
    "        self.n = n\n",
    "    \n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        \n",
    "        # compute token proabilities given counts\n",
    "        self.probs = defaultdict(Counter)\n",
    "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
    "        \n",
    "        # populate self.probs with actual probabilities\n",
    "        for ngram, tokens_count in counts.items():\n",
    "            total = sum(tokens_count.values())\n",
    "            for token, count in tokens_count.items():\n",
    "                self.probs[ngram][token] = count / total\n",
    "            \n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
    "        \"\"\"\n",
    "        prefix = prefix.split()\n",
    "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
    "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
    "        return self.probs[tuple(prefix)]\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :param next_token: the next token to predict probability for\n",
    "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
    "        \"\"\"\n",
    "        return self.get_possible_next_tokens(prefix).get(next_token, 0)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "0ftnn4nmuzrup6c0vvhb8q"
   },
   "source": [
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "a7zajcnvhqupvcrmacvkur",
    "ExecuteTime": {
     "end_time": "2024-12-29T09:36:44.579172Z",
     "start_time": "2024-12-29T09:36:44.560951Z"
    }
   },
   "source": [
    "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
    "\n",
    "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
    "assert np.allclose(p_initial['learning'], 0.02)\n",
    "assert np.allclose(p_initial['a'], 0.13)\n",
    "assert np.allclose(p_initial.get('meow', 0), 0)\n",
    "assert np.allclose(sum(p_initial.values()), 1)\n",
    "\n",
    "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
    "assert np.allclose(p_a['machine'], 0.15384615)\n",
    "assert np.allclose(p_a['note'], 0.23076923)\n",
    "assert np.allclose(p_a.get('the', 0), 0)\n",
    "assert np.allclose(sum(p_a.values()), 1)\n",
    "\n",
    "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
    "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
    "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
    "    \"your 3-gram model should only depend on 2 previous words\""
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 15823.38it/s]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "oh8r9a41kuk4r51wra9"
   },
   "source": [
    "Now that you've got a working n-gram language model, let's see what sequences it can generate. But first, let's train it on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "f17xoejjppmooo2nopw4xo",
    "ExecuteTime": {
     "end_time": "2024-12-29T09:38:30.020047Z",
     "start_time": "2024-12-29T09:38:17.577360Z"
    }
   },
   "source": "lm = NGramLanguageModel(lines, n=3)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41000/41000 [00:09<00:00, 4533.60it/s]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T09:53:58.927455Z",
     "start_time": "2024-12-27T09:53:58.918077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prefix = \"reinforcement learning\"\n",
    "\n",
    "for i in range(100):\n",
    "    tokens, probs = zip(*lm.get_possible_next_tokens(prefix).items())\n",
    "    next_token = np.random.choice(tokens, p=probs)\n",
    "    prefix = prefix + ' ' + next_token\n",
    "    \n",
    "    if next_token == EOS or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "\n",
    "print(prefix)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinforcement learning based argument component detection ; argument component detection ( acd ) is an important theorem known for propositional logic and first - order denoising ; let $ u \\ in \\ mbox { bv } _ \\ mbox { loc }(\\ omega ; \\ mathbb { r } _ +^* \\ times \\ mathbb { s } _ \\ ell : \\ w_ { s }( h ) \\ geq \\ lambda \\}$, for some $\\ lambda > 0 $. the time complexity is just o ( n ). we have so far found no class of graphs correlated with\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "reinforcement learning based argument component detection ; argument component detection ( acd ) is an important theorem known for propositional logic and first - order denoising ; let $ u \\ in \\ mbox { bv } _ \\ mbox { loc }(\\ omega ; \\ mathbb { r } _ +^* \\ times \\ mathbb { s } _ \\ ell : \\ w_ { s }( h ) \\ geq \\ lambda \\}$, for some $\\ lambda > 0 $. the time complexity is just o ( n ). we have so far found no class of graphs correlated with"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2kd9glwnkr470qc4bt7f1e"
   },
   "source": [
    "The process of generating sequences is... well, it's sequential. You maintain a list of tokens and iteratively add next token by sampling with probabilities.\n",
    "\n",
    "$ X = [] $\n",
    "\n",
    "__forever:__\n",
    "* $w_{next} \\sim P(w_{next} | X)$\n",
    "* $X = concat(X, w_{next})$\n",
    "\n",
    "\n",
    "Instead of sampling with probabilities, one can also try always taking most likely token, sampling among top-K most likely tokens or sampling with temperature. In the latter case (temperature), one samples from\n",
    "\n",
    "$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$$\n",
    "\n",
    "Where $\\tau > 0$ is model temperature. If $\\tau << 1$, more likely tokens will be sampled with even higher probability while less likely tokens will vanish."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "sgbatlm9vzb4z889fho7",
    "ExecuteTime": {
     "end_time": "2024-12-29T09:38:32.826795Z",
     "start_time": "2024-12-29T09:38:32.822888Z"
    }
   },
   "source": [
    "def get_next_token(lm: NGramLanguageModel, prefix, temperature=1.0):\n",
    "    \"\"\"\n",
    "    return next token after prefix;\n",
    "    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n",
    "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
    "    \"\"\"\n",
    "    probabilities = lm.get_possible_next_tokens(prefix) \n",
    "    \n",
    "    if temperature == 0:\n",
    "        return max(probabilities, key=probabilities.get)\n",
    "    \n",
    "    tokens, probs = zip(*probabilities.items())\n",
    "    \n",
    "    adjusted_probs = [prob**(1 / temperature) for prob in probs]\n",
    "    \n",
    "    total = sum(adjusted_probs)\n",
    "    normalized_probs = [prob / total for prob in adjusted_probs]\n",
    "\n",
    "    return np.random.choice(tokens, p=normalized_probs)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "98l40131wjtd5xbdm5b2nr",
    "ExecuteTime": {
     "end_time": "2024-12-29T09:38:36.597176Z",
     "start_time": "2024-12-29T09:38:33.248162Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
    "assert 250 < test_freqs['not'] < 450\n",
    "assert 8500 < test_freqs['been'] < 9500\n",
    "assert 1 < test_freqs['lately'] < 200\n",
    "\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
    "assert 1500 < test_freqs['learning'] < 3000\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
    "assert 8000 < test_freqs['learning'] < 9000\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
    "assert test_freqs['learning'] == 10000\n",
    "\n",
    "print(\"Looks nice!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks nice!\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ux4n8iq523n4s3ftrelhxj"
   },
   "source": [
    "Let's have fun with this model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "1nnnycga61rijt6nd8zai",
    "ExecuteTime": {
     "end_time": "2024-12-29T09:40:32.397855Z",
     "start_time": "2024-12-29T09:40:32.363741Z"
    }
   },
   "source": [
    "prefix = 'reinforcement learning' # <- your ideas :)\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix, temperature=1)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinforcement learning framework for abnormal event detection , unmixing , and non - probabilistic point and line of research in deep feed - forward neural network hardware architecture for this it is a comprehensive study of complicated control systems are increasingly used in many physical processes which underlie observed black - box settings . in our formalization by introducing the discriminative component and a lot of interest . conversely , the extracted features to classify the original curve evolution , being more visually stable and high - throughput mrna sequencing ( scrna - seq data . in this paper , we formulate\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "pxyjsv3b7r8thdfxlgitl",
    "ExecuteTime": {
     "end_time": "2024-12-27T10:21:17.308466Z",
     "start_time": "2024-12-27T10:21:17.274488Z"
    }
   },
   "source": [
    "prefix = 'reinforcement learning' # <- more of your ideas\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinforcement learning ( rl ) has been compared with the aim is to find a path forward for developing a method for the first time , the number of samples , and the results of our approach is a well - known approach to a specific way . we show that the proposed method . the proposed approach . _EOS_\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2n90bscmzfko0qnctp7ysc"
   },
   "source": [
    "__More in the homework:__ nucleus sampling, top-k sampling, beam search(not for the faint of heart)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3gdmey7g8at5n5c5x4gayh"
   },
   "source": [
    "### Evaluating language models: perplexity (1point)\n",
    "\n",
    "Perplexity is a measure of how well your model approximates the true probability distribution behind the data. __Smaller perplexity = better model__.\n",
    "\n",
    "To compute perplexity on one sentence, use:\n",
    "$$\n",
    "    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n",
    "$$\n",
    "\n",
    "\n",
    "On the corpora level, perplexity is a product of probabilities of all tokens in all sentences to the power of $1/N$, where $N$ is __total length (in tokens) of all sentences__ in corpora.\n",
    "\n",
    "This number can quickly get too small for float32/float64 precision, so we recommend you to first compute log-perplexity (from log-probabilities) and then take the exponent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "5hp010xyzzb4vqewo1bhny",
    "ExecuteTime": {
     "end_time": "2024-12-29T12:09:05.324980Z",
     "start_time": "2024-12-29T12:09:05.315199Z"
    }
   },
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "def perplexity(lm: NGramLanguageModel, lines, min_logprob=np.log(10 ** -50.)):\n",
    "    \"\"\"\n",
    "    :param lines: a list of strings with space-separated tokens\n",
    "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n",
    "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
    "    \n",
    "    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n",
    "    \n",
    "    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    min_logprob_exp = np.exp(min_logprob)\n",
    "    \n",
    "    line_tokens = list(map(lambda line: line.split() + [EOS], lines))\n",
    "    total_tokens = sum(map(len, line_tokens))\n",
    "    total_logprob = 0.0\n",
    "    \n",
    "    for tokens in tqdm(line_tokens):\n",
    "        \n",
    "        queue = deque([UNK] * (lm.n - 1), maxlen=lm.n - 1)\n",
    "        \n",
    "        for token in tokens:\n",
    "            prefix = \" \".join(queue)\n",
    "            \n",
    "            prob = lm.get_next_token_prob(prefix, token)\n",
    "            \n",
    "            logprob = np.log(max(prob, min_logprob_exp))\n",
    "            total_logprob += logprob\n",
    "            \n",
    "            queue.append(token)\n",
    "        \n",
    "    avg_neg_logprob = -total_logprob / total_tokens\n",
    "    \n",
    "    return np.exp(avg_neg_logprob)"
   ],
   "outputs": [],
   "execution_count": 208
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "8b689bobhkey04x7pabupj",
    "ExecuteTime": {
     "end_time": "2024-12-29T12:09:05.849601Z",
     "start_time": "2024-12-29T12:09:05.776100Z"
    }
   },
   "source": [
    "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
    "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
    "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
    "\n",
    "ppx1 = perplexity(lm1, dummy_lines)\n",
    "ppx3 = perplexity(lm3, dummy_lines)\n",
    "ppx10 = perplexity(lm10, dummy_lines)\n",
    "ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n",
    "\n",
    "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
    "\n",
    "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be non-negative and reasonably small\"\n",
    "assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n",
    "assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n",
    "    \" Make sure you use min_logprob right\"\n",
    "assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 100/100 [00:00<00:00, 66062.44it/s]\n",
      "Counting ngrams: 100%|██████████| 100/100 [00:00<00:00, 26430.80it/s]\n",
      "Counting ngrams: 100%|██████████| 100/100 [00:00<00:00, 14310.63it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 11764.24it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 10977.55it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 8990.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5433.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities: ppx1=318.213 ppx3=1.520 ppx10=1.184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 209
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ypc4lks4vs1li908fqi8"
   },
   "source": [
    "Now let's measure the actual perplexity: we'll split the data into train and test and score model on test data only."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "tjnehsem2lmijkg2lto4w",
    "ExecuteTime": {
     "end_time": "2024-12-29T12:09:39.001872Z",
     "start_time": "2024-12-29T12:09:07.741195Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
    "\n",
    "for n in (1, 2, 3):\n",
    "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 30750/30750 [00:02<00:00, 14518.95it/s]\n",
      "100%|██████████| 10250/10250 [00:03<00:00, 3123.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 1832.23136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 30750/30750 [00:03<00:00, 9158.17it/s]\n",
      "100%|██████████| 10250/10250 [00:04<00:00, 2270.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 85653987.28774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 30750/30750 [00:07<00:00, 4351.15it/s]\n",
      "100%|██████████| 10250/10250 [00:06<00:00, 1707.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 61999196259043346743296.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 210
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "38nfbfkpzgfxik8kccyt1l"
   },
   "outputs": [],
   "source": [
    "# whoops, it just blew up :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "oopn2o57wxm9vbxzycytce"
   },
   "source": [
    "### LM Smoothing\n",
    "\n",
    "The problem with our simple language model is that whenever it encounters an n-gram it has never seen before, it assigns it with the probabilitiy of 0. Every time this happens, perplexity explodes.\n",
    "\n",
    "To battle this issue, there's a technique called __smoothing__. The core idea is to modify counts in a way that prevents probabilities from getting too low. The simplest algorithm here is Additive smoothing (aka [Lapace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)):\n",
    "\n",
    "$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$\n",
    "\n",
    "If counts for a given prefix are low, additive smoothing will adjust probabilities to a more uniform distribution. Not that the summation in the denominator goes over _all words in the vocabulary_.\n",
    "\n",
    "Here's an example code we've implemented for you:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "ioh26rlov6g8l2ssj1c8pm",
    "ExecuteTime": {
     "end_time": "2024-12-29T12:09:39.581114Z",
     "start_time": "2024-12-29T12:09:39.574446Z"
    }
   },
   "source": [
    "class LaplaceLanguageModel(NGramLanguageModel): \n",
    "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
    "    def __init__(self, lines, n, delta=1.0):\n",
    "        self.n = n\n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
    "        self.probs = defaultdict(Counter)\n",
    "\n",
    "        for prefix in counts:\n",
    "            token_counts = counts[prefix]\n",
    "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
    "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
    "                                          for token in token_counts}\n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "        missing_prob_total = max(0, missing_prob_total)\n",
    "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        if next_token in token_probs:\n",
    "            return token_probs[next_token]\n",
    "        else:\n",
    "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
    "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))        "
   ],
   "outputs": [],
   "execution_count": 211
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "90vsann3920ie05r2blbmi",
    "execution_id": "3868303d-0bb9-42c6-a9a8-dcf485c8220c"
   },
   "source": [
    "**Disclaimer**: the implementation above assumes all words unknown within a given context to be equally likely, *as well as the words outside of vocabulary*. Therefore, its' perplexity will be lower than it should when encountering such words. Therefore, comparing it with a model with fewer unknown words will not be fair. When implementing your own smoothing, you may handle this by adding a virtual `UNK` token of non-zero probability. Technically, this will result in a model where probabilities do not add up to $1$, but it is close enough for a practice excercise."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "3xvxkdxcmfqucruyt66mdc",
    "ExecuteTime": {
     "end_time": "2024-12-29T12:09:41.090798Z",
     "start_time": "2024-12-29T12:09:41.062480Z"
    }
   },
   "source": [
    "#test that it's a valid probability model\n",
    "for n in (1, 2, 3):\n",
    "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
    "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 100/100 [00:00<00:00, 47105.84it/s]\n",
      "Counting ngrams: 100%|██████████| 100/100 [00:00<00:00, 48033.72it/s]\n",
      "Counting ngrams: 100%|██████████| 100/100 [00:00<00:00, 37193.44it/s]\n"
     ]
    }
   ],
   "execution_count": 212
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "j6zqa50koitjjri9ipd8ec",
    "ExecuteTime": {
     "end_time": "2024-12-29T12:12:00.676854Z",
     "start_time": "2024-12-29T12:11:19.993736Z"
    }
   },
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 30750/30750 [00:02<00:00, 12854.75it/s]\n",
      "100%|██████████| 10250/10250 [00:09<00:00, 1111.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 977.67559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 30750/30750 [00:03<00:00, 8844.82it/s]\n",
      "100%|██████████| 10250/10250 [00:07<00:00, 1290.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 470.48021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 30750/30750 [00:06<00:00, 4485.95it/s]\n",
      "100%|██████████| 10250/10250 [00:07<00:00, 1420.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 3679.44765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 215
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "pjuqt30jcerwbz1ym9zv1",
    "ExecuteTime": {
     "end_time": "2024-12-29T11:17:30.541657Z",
     "start_time": "2024-12-29T11:17:30.291691Z"
    }
   },
   "source": [
    "# optional: try to sample tokens from such a model\n",
    "\n",
    "tokens, probs = zip(*lm.get_possible_next_tokens(\"\").items())\n",
    "np.random.choice(tokens, p=list(probs))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'monocular'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 132
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3b8s1y9uls4fosu3yp28gg"
   },
   "source": [
    "### Kneser-Ney smoothing (2 points)\n",
    "\n",
    "Additive smoothing is simple, reasonably good but definitely not a State of The Art algorithm.\n",
    "\n",
    "\n",
    "Your final task in this notebook is to implement [Kneser-Ney](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing) smoothing.\n",
    "\n",
    "It can be computed recurrently, for n>1:\n",
    "\n",
    "$$P_{kn}(w_t | prefix_{n-1}) = { \\max(0, Count(prefix_{n-1}, w_t) - \\delta) \\over \\sum_{\\hat w} Count(prefix_{n-1}, \\hat w)} + \\lambda_{prefix_{n-1}} \\cdot P_{kn}(w_t | prefix_{n-2})$$\n",
    "\n",
    "where\n",
    "- $prefix_{n-1}$ is a tuple of {n-1} previous tokens\n",
    "- $lambda_{prefix_{n-1}}$ is a normalization constant chosen so that probabilities add up to 1\n",
    "- Unigram $P_{kn}(w_t | prefix_{n-2})$ corresponds to Kneser Ney smoothing for {N-1}-gram language model.\n",
    "- Unigram $P_{kn}(w_t)$ is a special case: how likely it is to see x_t in an unfamiliar context\n",
    "\n",
    "See lecture slides or wiki for more detailed formulae.\n",
    "\n",
    "__Your task__ is to\n",
    "- implement `KneserNeyLanguageModel` class,\n",
    "- test it on 1-3 gram language models\n",
    "- find optimal (within reason) smoothing delta for 3-gram language model with Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "2ix7kzw02v30oye55322all",
    "ExecuteTime": {
     "end_time": "2024-12-29T12:12:04.447862Z",
     "start_time": "2024-12-29T12:12:04.432334Z"
    }
   },
   "source": [
    "class KneserNeyLanguageModel(NGramLanguageModel): \n",
    "    \"\"\" A template for Kneser-Ney language model. Default delta may be suboptimal. \"\"\"\n",
    "    def __init__(self, lines: list[str], n, delta=1.0):\n",
    "        self.n = n\n",
    "        self.delta = delta\n",
    "        \n",
    "        ngrams_counts = count_ngrams(lines, self.n, tqdm_desc=\"Counting ngrams\")\n",
    "        lower_ngrams_counts = count_ngrams(lines, self.n - 1, tqdm_desc=\"Counting lower ngrams\")\n",
    "                    \n",
    "        self.ngrams_probs = self._compute_ngrams_probs(ngrams_counts, delta)\n",
    "        self.lower_ngrams_probs = self._compute_ngrams_probs(lower_ngrams_counts, delta=0)\n",
    "        self.lambdas = self._compute_lambdas(ngrams_counts, delta)\n",
    "        \n",
    "        self.vocab = set(token for token_counts in ngrams_counts.values() for token in token_counts)\n",
    "        self.vocab.add(UNK)\n",
    "        \n",
    "        self.probs = self._compute_token_prob_for_prefix()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compute_lambdas(ngrams_counts, delta):\n",
    "        lambdas = defaultdict(float)\n",
    "        for prefix, token_counts in ngrams_counts.items():\n",
    "            total_token_count = sum(token_counts.values())\n",
    "            if total_token_count > 0:\n",
    "                    lambdas[prefix] = (delta * len(token_counts)) / total_token_count\n",
    "            else:\n",
    "                lambdas[prefix] = 0\n",
    "        return lambdas\n",
    "        \n",
    "    @staticmethod\n",
    "    def _compute_ngrams_probs(ngrams_counts, delta):\n",
    "        ngrams_probs = defaultdict(dict)\n",
    "        for prefix, token_counts in ngrams_counts.items():\n",
    "            total_token_count = sum(token_counts.values())\n",
    "            for token, count in token_counts.items():\n",
    "                if total_token_count > 0:\n",
    "                    ngrams_probs[prefix][token] = max(0, count - delta) / total_token_count\n",
    "                else :\n",
    "                    ngrams_probs[prefix][token] = 0\n",
    "        return ngrams_probs\n",
    "    \n",
    "    def _compute_token_prob_for_prefix(self):\n",
    "        probs = defaultdict(dict)\n",
    "        for prefix, token_probs in tqdm(self.ngrams_probs.items(), desc=\"Computing token probabilities\"):\n",
    "            \n",
    "            for token, prob in token_probs.items():\n",
    "                probs[prefix][token] = prob + self.lambdas[prefix] * self.lower_ngrams_probs[prefix[1:]][token]\n",
    "            \n",
    "            missing_prob_total = 1.0 - sum(probs[prefix].values())\n",
    "            missing_prob_total = max(0, missing_prob_total)\n",
    "            missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "            probs[prefix][UNK] = missing_prob\n",
    "            \n",
    "        return probs\n",
    "\n",
    "    def get_possible_next_tokens(self, prefix, tuple_prefix=False):\n",
    "        if tuple_prefix:\n",
    "            token_probs = self.probs[prefix]\n",
    "        else:\n",
    "            token_probs = super().get_possible_next_tokens(prefix)\n",
    "        \n",
    "        if not token_probs:\n",
    "            token_probs[UNK] = 1 / len(self.vocab)\n",
    "        \n",
    "        return token_probs\n",
    "        \n",
    "    def get_next_token_prob(self, prefix, next_token, tuple_prefix=False):\n",
    "        token_probs = self.get_possible_next_tokens(prefix, tuple_prefix=tuple_prefix)\n",
    "        if next_token in token_probs:\n",
    "            return token_probs[next_token]\n",
    "        return token_probs[UNK]"
   ],
   "outputs": [],
   "execution_count": 216
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "lsk91832qbmdt7x1q0a8z4",
    "ExecuteTime": {
     "end_time": "2024-12-29T12:33:24.639349Z",
     "start_time": "2024-12-29T12:33:24.579163Z"
    }
   },
   "source": [
    "#test that it's a valid probability model\n",
    "for n in (1, 2, 3):\n",
    "    dummy_lm = KneserNeyLanguageModel(dummy_lines, n=n)\n",
    "    next_token_probs = [dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]\n",
    "    max_prob_id = np.argmax(next_token_probs)\n",
    "    print(list(dummy_lm.vocab)[max_prob_id], next_token_probs[max_prob_id])\n",
    "    assert np.allclose(sum(next_token_probs), 1), \"I told you not to break anything! :)\""
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 100/100 [00:00<00:00, 47771.12it/s]\n",
      "Counting lower ngrams: 100%|██████████| 100/100 [00:00<00:00, 48433.07it/s]\n",
      "Computing token probabilities: 100%|██████████| 1/1 [00:00<00:00, 2248.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.05415368058175727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 100/100 [00:00<00:00, 46192.78it/s]\n",
      "Counting lower ngrams: 100%|██████████| 100/100 [00:00<00:00, 81872.03it/s]\n",
      "Computing token probabilities: 100%|██████████| 946/946 [00:00<00:00, 291236.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new 0.07557414990208296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 100/100 [00:00<00:00, 18731.26it/s]\n",
      "Counting lower ngrams: 100%|██████████| 100/100 [00:00<00:00, 55195.47it/s]\n",
      "Computing token probabilities: 100%|██████████| 2086/2086 [00:00<00:00, 491093.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note 0.18198874296435272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 250
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:28:20.652461Z",
     "start_time": "2024-12-29T12:28:20.648309Z"
    }
   },
   "cell_type": "code",
   "source": "np.equal([1, 2, 3], 1)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 233
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:12:05.397255Z",
     "start_time": "2024-12-29T12:12:05.392457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "def perplexity(lm: KneserNeyLanguageModel, lines, min_logprob=np.log(10 ** -50.)):\n",
    "    \"\"\"\n",
    "    :param lines: a list of strings with space-separated tokens\n",
    "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n",
    "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
    "    \n",
    "    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n",
    "    \n",
    "    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    min_logprob_exp = np.exp(min_logprob)\n",
    "    \n",
    "    line_tokens = list(map(lambda line: line.split() + [EOS], lines))\n",
    "    total_tokens = sum(map(len, line_tokens))\n",
    "    total_logprob = 0.0\n",
    "    \n",
    "    for tokens in tqdm(line_tokens, desc=\"Computing perplexity\"):\n",
    "        \n",
    "        queue = deque([UNK] * (lm.n - 1), maxlen=lm.n - 1)\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            prefix = tuple(queue)\n",
    "            \n",
    "            prob = lm.get_next_token_prob(prefix, token, tuple_prefix=True)\n",
    "            \n",
    "            logprob = np.log(max(prob, min_logprob_exp))\n",
    "            total_logprob += logprob\n",
    "            \n",
    "            queue.append(token)\n",
    "        \n",
    "    avg_neg_logprob = -total_logprob / total_tokens\n",
    "    \n",
    "    return np.exp(avg_neg_logprob)"
   ],
   "outputs": [],
   "execution_count": 218
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellId": "pp3jtkk9annp1qkou58x1b",
    "ExecuteTime": {
     "end_time": "2024-12-29T12:14:14.371181Z",
     "start_time": "2024-12-29T12:13:35.185697Z"
    }
   },
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = KneserNeyLanguageModel(train_lines, n=n, delta=0.7)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 30750/30750 [00:02<00:00, 12556.58it/s]\n",
      "Counting lower ngrams: 100%|██████████| 30750/30750 [00:02<00:00, 14802.74it/s]\n",
      "Computing token probabilities: 100%|██████████| 1/1 [00:00<00:00, 35.51it/s]\n",
      "Computing perplexity: 100%|██████████| 10250/10250 [00:02<00:00, 4131.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 974.32977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 30750/30750 [00:03<00:00, 8845.25it/s]\n",
      "Counting lower ngrams: 100%|██████████| 30750/30750 [00:02<00:00, 15212.21it/s]\n",
      "Computing token probabilities: 100%|██████████| 54176/54176 [00:00<00:00, 101250.95it/s]\n",
      "Computing perplexity: 100%|██████████| 10250/10250 [00:03<00:00, 3096.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 252.28038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting ngrams: 100%|██████████| 30750/30750 [00:06<00:00, 4490.05it/s]\n",
      "Counting lower ngrams: 100%|██████████| 30750/30750 [00:03<00:00, 9198.07it/s]\n",
      "Computing token probabilities: 100%|██████████| 1005464/1005464 [00:03<00:00, 286423.50it/s]\n",
      "Computing perplexity: 100%|██████████| 10250/10250 [00:03<00:00, 2638.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 1000.87248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 221
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
  "notebookPath": "seminar.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
